/*
 * Copyright (c) 2016-2018 Wuklab, Purdue University. All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 */

#include <lego/mm.h>
#include <lego/slab.h>
#include <lego/sched.h>
#include <lego/kernel.h>
#include <lego/ptrace.h>
#include <lego/memory.h>
#include <lego/signal.h>
#include <lego/uaccess.h>
#include <lego/extable.h>
#include <lego/pgfault.h>
#include <lego/profile.h>
#include <processor/processor.h>

#include <asm/asm.h>
#include <asm/page.h>
#include <asm/traps.h>
#include <asm/pgtable.h>
#include <asm/kdebug.h>
#include <asm/vsyscall.h>

#ifdef CONFIG_COMP_MEMORY
#include <teleport/pushdown.h>
#endif

/*
 * Page fault error code bits:
 *
 *   bit 0 ==	 0: no page found	1: protection fault
 *   bit 1 ==	 0: read access		1: write access
 *   bit 2 ==	 0: kernel-mode access	1: user-mode access
 *   bit 3 ==				1: use of reserved bit detected
 *   bit 4 ==				1: fault was an instruction fetch
 *   bit 5 ==				1: protection keys block access
 */
enum x86_pf_error_code {
    PF_PROT		=		1 << 0,
    PF_WRITE	=		1 << 1,
    PF_USER		=		1 << 2,
    PF_RSVD		=		1 << 3,
    PF_INSTR	=		1 << 4,
    PF_PK		=		1 << 5,
};

static int fault_in_kernel_space(unsigned long address)
{
    return address >= TASK_SIZE_MAX;
}

/*
 * Prefetch quirks:
 *
 * 32-bit mode:
 *
 *   Sometimes AMD Athlon/Opteron CPUs report invalid exceptions on prefetch.
 *   Check that here and ignore it.
 *
 * 64-bit mode:
 *
 *   Sometimes the CPU reports invalid exceptions on prefetch.
 *   Check that here and ignore it.
 *
 * Opcode checker based on code by Richard Brunner.
 */
    static inline int
check_prefetch_opcode(struct pt_regs *regs, unsigned char *instr,
        unsigned char opcode, int *prefetch)
{
    unsigned char instr_hi = opcode & 0xf0;
    unsigned char instr_lo = opcode & 0x0f;

    switch (instr_hi) {
        case 0x20:
        case 0x30:
            /*
             * Values 0x26,0x2E,0x36,0x3E are valid x86 prefixes.
             * In X86_64 long mode, the CPU will signal invalid
             * opcode if some of these prefixes are present so
             * X86_64 will never get here anyway
             */
            return ((instr_lo & 7) == 0x6);
#ifdef CONFIG_X86_64
        case 0x40:
            /*
             * In AMD64 long mode 0x40..0x4F are valid REX prefixes
             * Need to figure out under what instruction mode the
             * instruction was issued. Could check the LDT for lm,
             * but for now it's good enough to assume that long
             * mode only uses well known segments or kernel.
             */
            return (!user_mode(regs) || user_64bit_mode(regs));
#endif
        case 0x60:
            /* 0x64 thru 0x67 are valid prefixes in all modes. */
            return (instr_lo & 0xC) == 0x4;
        case 0xF0:
            /* 0xF0, 0xF2, 0xF3 are valid prefixes in all modes. */
            return !instr_lo || (instr_lo>>1) == 1;
        case 0x00:
            /* Prefetch instruction is 0x0F0D or 0x0F18 */
            if (probe_kernel_address(instr, opcode))
                return 0;

            *prefetch = (instr_lo == 0xF) &&
                (opcode == 0x0D || opcode == 0x18);
            return 0;
        default:
            return 0;
    }
}

    static inline unsigned long
convert_ip_to_linear(struct task_struct *child, struct pt_regs *regs)
{
    return regs->ip;
}

    static int
is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)
{
    unsigned char *max_instr;
    unsigned char *instr;
    int prefetch = 0;

    /*
     * If it was a exec (instruction fetch) fault on NX page, then
     * do not ignore the fault:
     */
    if (error_code & PF_INSTR)
        return 0;

    instr = (void *)convert_ip_to_linear(current, regs);
    max_instr = instr + 15;

    if (user_mode(regs) && instr >= (unsigned char *)TASK_SIZE_MAX)
        return 0;

    while (instr < max_instr) {
        unsigned char opcode;

        if (probe_kernel_address(instr, opcode))
            break;

        instr++;

        if (!check_prefetch_opcode(regs, instr, opcode, &prefetch))
            break;
    }
    return prefetch;
}

static void dump_pagetable(unsigned long address)
{
    pgd_t *base = __va(read_cr3() & PHYSICAL_PAGE_MASK);
    pgd_t *pgd = base + pgd_index(address);
    pud_t *pud;
    pmd_t *pmd;
    pte_t *pte;

    printk("PGD %lx ", pgd_val(*pgd));

    if (!pgd_present(*pgd))
        goto out;

    pud = pud_offset(pgd, address);

    pr_cont("PUD %lx ", pud_val(*pud));
    if (!pud_present(*pud) || pud_large(*pud))
        goto out;

    pmd = pmd_offset(pud, address);

    pr_cont("PMD %lx ", pmd_val(*pmd));
    if (!pmd_present(*pmd) || pmd_large(*pmd))
        goto out;

    pte = pte_offset_kernel(pmd, address);

    pr_cont("PTE %lx", pte_val(*pte));
out:
    pr_cont("\n");
    return;
}

static int spurious_fault_check(unsigned long error_code, pte_t *pte)
{
    if ((error_code & PF_WRITE) && !pte_write(*pte))
        return 0;

    if ((error_code & PF_INSTR) && !pte_exec(*pte))
        return 0;
    /*
     * Note: We do not do lazy flushing on protection key
     * changes, so no spurious fault will ever set PF_PK.
     */
    if ((error_code & PF_PK))
        return 1;

    return 1;
}

/*
 * Handle a spurious fault caused by a stale TLB entry.
 *
 * This allows us to lazily refresh the TLB when increasing the
 * permissions of a kernel page (RO -> RW or NX -> X).  Doing it
 * eagerly is very expensive since that implies doing a full
 * cross-processor TLB flush, even if no stale TLB entries exist
 * on other processors.
 *
 * Spurious faults may only occur if the TLB contains an entry with
 * fewer permission than the page table entry.  Non-present (P = 0)
 * and reserved bit (R = 1) faults are never spurious.
 *
 * There are no security implications to leaving a stale TLB when
 * increasing the permissions on a page.
 *
 * Returns non-zero if a spurious fault was handled, zero otherwise.
 *
 * See Intel Developer's Manual Vol 3 Section 4.10.4.3, bullet 3
 * (Optional Invalidation).
 */
    static noinline int
spurious_fault(unsigned long error_code, unsigned long address)
{
    pgd_t *pgd;
    pud_t *pud;
    pmd_t *pmd;
    pte_t *pte;
    int ret;

    /*
     * Only writes to RO or instruction fetches from NX may cause
     * spurious faults.
     *
     * These could be from user or supervisor accesses but the TLB
     * is only lazily flushed after a kernel mapping protection
     * change, so user accesses are not expected to cause spurious
     * faults.
     */
    if (error_code != (PF_WRITE | PF_PROT)
            && error_code != (PF_INSTR | PF_PROT))
        return 0;

    pgd = init_mm.pgd + pgd_index(address);
    if (!pgd_present(*pgd))
        return 0;

    pud = pud_offset(pgd, address);
    if (!pud_present(*pud))
        return 0;

    if (pud_large(*pud))
        return spurious_fault_check(error_code, (pte_t *) pud);

    pmd = pmd_offset(pud, address);
    if (!pmd_present(*pmd))
        return 0;

    if (pmd_large(*pmd))
        return spurious_fault_check(error_code, (pte_t *) pmd);

    pte = pte_offset_kernel(pmd, address);
    if (!pte_present(*pte))
        return 0;

    ret = spurious_fault_check(error_code, pte);
    if (!ret)
        return 0;

    /*
     * Make sure we have permissions in PMD.
     * If not, then there's a bug in the page tables:
     */
    ret = spurious_fault_check(error_code, (pte_t *) pmd);
    WARN_ONCE(!ret, "PMD has incorrect permission bits\n");

    return ret;
}
static noinline int vmalloc_fault(unsigned long address)
{
    pgd_t *pgd, *pgd_ref;
    pud_t *pud, *pud_ref;
    pmd_t *pmd, *pmd_ref;
    pte_t *pte, *pte_ref;

    /* Make sure we are in vmalloc area: */
    if (!(address >= VMALLOC_START && address < VMALLOC_END))
        return -1;

    /*
     * Copy kernel mappings over when needed. This can also
     * happen within a race in page table update. In the later
     * case just flush:
     */
    pgd = (pgd_t *)__va(read_cr3()) + pgd_index(address);
    pgd_ref = pgd_offset_k(address);
    if (pgd_none(*pgd_ref))
        return -1;

    if (pgd_none(*pgd)) {
        pgd_set(pgd, *pgd_ref);
    } else {
        BUG_ON(pgd_page_vaddr(*pgd) != pgd_page_vaddr(*pgd_ref));
    }

    /*
     * Below here mismatches are bugs because these lower tables
     * are shared:
     */

    pud = pud_offset(pgd, address);
    pud_ref = pud_offset(pgd_ref, address);
    if (pud_none(*pud_ref))
        return -1;

    if (pud_none(*pud) || pud_pfn(*pud) != pud_pfn(*pud_ref))
        BUG();

    pmd = pmd_offset(pud, address);
    pmd_ref = pmd_offset(pud_ref, address);
    if (pmd_none(*pmd_ref))
        return -1;

    if (pmd_none(*pmd) || pmd_pfn(*pmd) != pmd_pfn(*pmd_ref))
        BUG();

    pte_ref = pte_offset_kernel(pmd_ref, address);
    if (!pte_present(*pte_ref))
        return -1;

    pte = pte_offset_kernel(pmd, address);

    /*
     * Don't use pte_page here, because the mappings can point
     * outside mem_map, and the NUMA hash lookup cannot handle
     * that:
     */
    if (!pte_present(*pte) || pte_pfn(*pte) != pte_pfn(*pte_ref))
        BUG();

    return 0;
}

static void show_fault_oops(struct pt_regs *regs, unsigned long error_code,
        unsigned long address)
{
    printk(KERN_ALERT "BUG: unable to handle kernel ");
    if (address < PAGE_SIZE)
        printk(KERN_CONT "NULL pointer dereference");
    else
        printk(KERN_CONT "paging request");

    printk(KERN_CONT   " at %p\n", (void *)address);
    printk(KERN_ALERT "IP: [<%p>] %pS\n", (void *)regs->ip, (void *)regs->ip);

    dump_pagetable(address);
}

    static void
force_sig_info_fault(int si_signo, int si_code, unsigned long address,
        struct task_struct *tsk, int fault)
{
    unsigned lsb = 0;
    siginfo_t info;

    info.si_signo	= si_signo;
    info.si_errno	= 0;
    info.si_code	= si_code;
    info.si_addr	= (void __user *)address;
    info.si_addr_lsb = lsb;

    force_sig_info(si_signo, &info, tsk);
}

#define task_stack_end_corrupted(task) \
    (*(end_of_stack(task)) != STACK_END_MAGIC)

/*
 * No user context attached.
 * There are only two possibilities:
 * 	fixup exception
 * 	kernel bug
 */
    static noinline void
no_context(struct pt_regs *regs, unsigned long error_code,
        unsigned long address, int signal, int si_code)
{
    struct task_struct *tsk = current;

    /* Are we prepared to handle this kernel fault? */
    if (fixup_exception(regs, X86_TRAP_PF)) {
        /*
         * Per the above we're !in_interrupt(), aka. task context.
         *
         * In this case we need to make sure we're not recursively
         * faulting through the emulate_vsyscall() logic.
         */
        if (current->thread.sig_on_uaccess_err && signal) {
            tsk->thread.trap_nr = X86_TRAP_PF;
            tsk->thread.error_code = error_code | PF_USER;
            tsk->thread.cr2 = address;

            force_sig_info_fault(signal, si_code, address,
                    tsk, 0);
        }

        /* Barring that, we can do the fixup and be happy. */
        return;
    }

    /*
     * 32-bit:
     *
     *   Valid to do another page fault here, because if this fault
     *   had been triggered by is_prefetch fixup_exception would have
     *   handled it.
     *
     * 64-bit:
     *
     *   Hall of shame of CPU/BIOS bugs.
     */
    if (is_prefetch(regs, error_code, address))
        return;

    /*
     * Well, kernel bugs!
     */

    show_fault_oops(regs, error_code, address);

    if (task_stack_end_corrupted(current))
        printk(KERN_EMERG "Thread overran stack, or stack corrupted\n");

    tsk->thread.cr2		= address;
    tsk->thread.trap_nr	= X86_TRAP_PF;
    tsk->thread.error_code	= error_code;

    __die("Oops", regs, error_code);

    /* Executive summary in case the body of the oops scrolled away */
    printk(KERN_DEFAULT "CR2: %016lx\n", address);
    panic("Fatal exception");
}

/*
 * Print out info about fatal segfaults, if the show_unhandled_signals
 * sysctl is set:
 */
    static inline void
show_signal_msg(struct pt_regs *regs, unsigned long error_code,
        unsigned long address, struct task_struct *tsk)
{
    printk("%s[%d]: segfault at %#lx ip %p sp %p error %lx\n",
            tsk->comm, tsk->pid, address,
            (void *)regs->ip, (void *)regs->sp, error_code);
}

    static noinline void
__bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
        unsigned long address, int si_code)
{
    struct task_struct *tsk = current;

    /* User mode accesses just cause a SIGSEGV */
    if (error_code & PF_USER) {
        /*
         * It's possible to have interrupts off here:
         */
        local_irq_enable();

        /*
         * Valid to do another page fault here because this one came
         * from user space:
         */
        if (is_prefetch(regs, error_code, address))
            return;

#ifdef CONFIG_X86_64
        /*
         * Instruction fetch faults in the vsyscall page might need
         * emulation.
         */
        if (unlikely((error_code & PF_INSTR) &&
                    ((address & ~0xfff) == VSYSCALL_ADDR))) {
            if (emulate_vsyscall(regs, address))
                return;
        }
#endif

        /*
         * To avoid leaking information about the kernel page table
         * layout, pretend that user-mode accesses to kernel addresses
         * are always protection faults.
         */
        if (address >= TASK_SIZE_MAX)
            error_code |= PF_PROT;

        show_signal_msg(regs, error_code, address, current);

        tsk->thread.cr2		= address;
        tsk->thread.error_code	= error_code;
        tsk->thread.trap_nr	= X86_TRAP_PF;

        force_sig_info_fault(SIGSEGV, si_code, address, tsk, 0);

        return;
    }

    no_context(regs, error_code, address, SIGSEGV, si_code);
}

    static noinline void
bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
        unsigned long address)
{
    __bad_area_nosemaphore(regs, error_code, address, SEGV_MAPERR);
}

    static void
do_sigbus(struct pt_regs *regs, unsigned long error_code,
        unsigned long address, unsigned int fault)
{
    struct task_struct *tsk = current;
    int code = BUS_ADRERR;

    /* Kernel mode? Handle exceptions or die: */
    if (!(error_code & PF_USER)) {
        no_context(regs, error_code, address, SIGBUS, BUS_ADRERR);
        return;
    }

    /* User-space => ok to do another page fault: */
    if (is_prefetch(regs, error_code, address))
        return;

    tsk->thread.cr2		= address;
    tsk->thread.error_code	= error_code;
    tsk->thread.trap_nr	= X86_TRAP_PF;

    force_sig_info_fault(SIGBUS, code, address, tsk, fault);
}

/*
 * pcache failed to handle this pgfault for some reasons.
 * Kill this user process or let fixup handle this.
 */
    static noinline void
mm_fault_error(struct pt_regs *regs, unsigned long error_code,
        unsigned long address, unsigned int fault)
{
    if (fatal_signal_pending(current) && !(error_code & PF_USER)) {
        no_context(regs, error_code, address, 0, 0);
        return;
    }

    if (fault & VM_FAULT_OOM) {
        /* Kernel mode? Handle exceptions or die: */
        if (!(error_code & PF_USER)) {
            no_context(regs, error_code, address, SIGSEGV, SEGV_MAPERR);
            return;
        }

        pr_info("CPU%d PID:%d fail to allocate pcache or victim cache lines.\n",
                smp_processor_id(), current->pid);
        bad_area_nosemaphore(regs, error_code, address);
    } else {
        if (fault & VM_FAULT_SIGBUS) {
            do_sigbus(regs, error_code, address, fault);
        } else if (fault & VM_FAULT_SIGSEGV) {
            bad_area_nosemaphore(regs, error_code, address);
        } else
            BUG();
    }
}

static void pgtable_bad(struct pt_regs *regs, unsigned long error_code,
        unsigned long address)
{
    struct task_struct *tsk = current;

    printk(KERN_ALERT "%d-%d-%s: Corrupted page table at address %lx\n",
            tsk->pid, tsk->group_leader->pid, tsk->comm, address);
    dump_pagetable(address);

    tsk->thread.cr2		= address;
    tsk->thread.trap_nr	= X86_TRAP_PF;
    tsk->thread.error_code	= error_code;

    __die("Bad pagetable", regs, error_code);
    panic("Fatal exception");
}

    static inline void
component_failure_check(struct pt_regs *regs, unsigned long error_code,
        unsigned long address)
{
#ifndef CONFIG_COMP_PROCESSOR
    /*
     * Memory component does not have any user context attached currently.
     */
    show_fault_oops(regs, error_code, address);
    __die("Oops", regs, error_code);
    printk(KERN_DEFAULT "CR2: %016lx\n", address);
    panic("Fatal exception");
#endif
}

DEFINE_PROFILE_POINT(pcache_handle_fault)

    /*
     * This routine handles page faults.  It determines the address,
     * and the problem, and then passes it off to one of the appropriate
     * routines.
     */
dotraplinkage void do_page_fault(struct pt_regs *regs, long error_code)
{
    int fault;
    unsigned long address = read_cr2();
    unsigned long flags = FAULT_FLAG_KILLABLE;
    PROFILE_POINT_TIME(pcache_handle_fault)

#ifdef CONFIG_COMP_MEMORY
    do {
        struct pushdown_instantiate_info *teleport_instance = NULL;
        if (error_code & PF_WRITE)
                flags |= FAULT_FLAG_WRITE;
        do {
            int i;
            for (i = 0; i != TELEPORT_PARALLELISM; i++) {
                if (teleport_instances[i] && (teleport_instances[i]->local_pid == current->pid)) {
                    teleport_instance = teleport_instances[i];
                    break;
                }
            }
        } while (false);
        if (teleport_instance) {
            struct pageinfo* curr_page;
            bool for_compute_node = false;
            unsigned long pg_address = address & PAGE_MASK;

            // check if this page fault is for compute pool
            if (teleport_instance->pageinfo_table) {
                hash_for_each_possible_with_bits(teleport_instance->pageinfo_table, curr_page, next, pg_address, teleport_instance->bits_to_hash) {
                    if (curr_page->addr == pg_address && curr_page->is_dirty) {
                        for_compute_node = true;
                        break;
                    }
                }
            }

            if (for_compute_node) {
                int res;
                res = teleport_handle_fault(current->mm, teleport_instance->pid, teleport_instance->tgid, pg_address, flags, teleport_instance->src_nid, !!(error_code & PF_WRITE), teleport_instance->is_coherent, teleport_instance);
                if (res) {
                    mm_fault_error(regs, error_code, address, fault);
                }
                return;
            }
            // handle the valid page faults here by emulating the page fault handler from compute pool
            fault = teleport_handle_fault_local(current->mm, teleport_instance->pid, teleport_instance->tgid, address, flags, teleport_instance->src_nid);
            if (fault) {
                mm_fault_error(regs, error_code, address, fault);
            }
            return;
        }
    } while (false);
#endif

    /*
     * We fault-in kernel-space virtual memory on-demand. The
     * 'reference' page table is init_mm.pgd.
     *
     * NOTE! We MUST NOT take any locks for this case. We may
     * be in an interrupt or a critical region, and should
     * only copy the information from the master page table,
     * nothing more.
     *
     * This verifies that the fault happens in kernel space
     * (error_code & 4) == 0, and that the fault was not a
     * protection error (error_code & 9) == 0.
     */
    // printk("fault_in_kernel_space: %d\n", fault_in_kernel_space(address));
    if (unlikely(fault_in_kernel_space(address))) {
        if (!(error_code & (PF_RSVD | PF_USER | PF_PROT))) {
            if (vmalloc_fault(address) >= 0)
                return;
        }

        /* Can handle a stale RO->RW TLB: */
        if (spurious_fault(error_code, address))
            return;

        bad_area_nosemaphore(regs, error_code, address);

        return;
    }

    if (unlikely(error_code & PF_RSVD))
        pgtable_bad(regs, error_code, address);

    /*
     * If we're in a region with pagefaults disabled,
     * then we must not take the fault, go straight to the fixup:
     */
    if (unlikely(faulthandler_disabled())) {
        bad_area_nosemaphore(regs, error_code, address);
        return;
    }

    /*
     * When running in the kernel, pgfault should only occur to
     * IP addresses listed in the exception table. The following
     * branch can catch normal kernel bugs.
     *
     * After this test, we are confirmed that the pgfault either came from
     * user mode, or kernel mode but with predefined fixup functions.
     */
    if (unlikely((error_code & PF_USER) == 0 &&
                !search_exception_tables(regs->ip))) {
        bad_area_nosemaphore(regs, error_code, address);
        return;
    }

    /*
     * It's safe to allow irq's after cr2 has been saved and the
     * vmalloc fault has been handled.
     *
     * User-mode registers count as a user access even for any
     * potential system fault or CPU buglet:
     */
    if (user_mode(regs)) {
        local_irq_enable();
        error_code |= PF_USER;
        flags |= FAULT_FLAG_USER;
    } else {
        if (regs->flags & X86_EFLAGS_IF)
            local_irq_enable();
    }

    if (error_code & PF_WRITE)
        flags |= FAULT_FLAG_WRITE;
    if (error_code & PF_INSTR)
        flags |= FAULT_FLAG_INSTRUCTION;

    /*
     * We've ruled out predefined kernel exceptions and kernel bugs.
     * Forward this pgfault to pcache, which will in turn ask remote memory
     * for further VM handling:
     */
    PROFILE_START(pcache_handle_fault);
    fault = pcache_handle_fault(current->mm, address, flags);
    PROFILE_LEAVE(pcache_handle_fault);
    if (unlikely(fault & VM_FAULT_ERROR)) {
        /*
         * If for any reason at all we couldn't handle the fault,
         * make sure we exit gracefully rather than endlessly redo the fault.
         */
        mm_fault_error(regs, error_code, address, fault);
        return;
    }
}
